# ROS/ALTH 検討メモ 03: 学習・評価プロトコル（安定化、成長、剪定）

作成日: 2025-12-29  
目的: 演算子を増やしても学習が壊れない手順と、研究として検証できる評価設計をまとめる。

---

## 0. 前提（本プロトコルの“縛り”）

- 演算子は **微分可能**
- 離散ルーティング（top-k等）の **明示的選択はしない**
- 層は「演算子全混合（連続係数）」で構成
- 混合前に **Norm** を入れてスケール崩壊を防ぐ

---

## 1. 学習するパラメータ範囲（おすすめ）

最初は「どこまで学習するか」を絞ると安定。

### 案A（まずは堅い）
- 学習: \(\{\beta_k\}\)（演算子の寄与度）＋最終ヘッド
- 固定: 演算子内部乱数 \(\omega_k\)

### 案B（少し表現を足す）
- 学習: \(\beta_k\) ＋「軽量アダプタ」 \(A_k\)（低ランク or 1x1 など）
- 固定: \(\omega_k\)

---

## 2. 初期化（増やしても壊れない）

### 2.1 係数 \(\beta\) の初期化
- 既存演算子: 小さめ（例: 0.01 近傍）または標準初期化
- **新規追加演算子**: \(\beta_{new}=0\) もしくは極小値  
  → 追加直後にモデルが壊れない

### 2.2 残差スケール \(\gamma\)
- \(\gamma\) を固定小値（例: 0.1）で開始し、必要なら学習させる
- 演算子数 \(K\) が増えるほど \(\gamma\) は小さくしたい（寄与総和が大きくなるため）

---

## 3. 正則化（“勝手に疎になる”を促す。ただし選択はしない）

### 3.1 係数への正則化
- \(L_1\): \(\lambda_1\sum_k |\beta_k|\)  
  → 演算子寄与を減らし、不要演算子が自然に小さくなる
- group-lasso（チャネル別\(\beta\)にする場合）
- 係数のエントロピー正則化（過度な集中を避けたい場合）

### 3.2 Operator Dropout（学習時）
- 学習ステップごとに演算子出力を確率 \(p\) でゼロ化  
  → 計算節約 + 一種のアンサンブル正則化  
  → ただし推論時は全混合（または剪定後のサブセット）

---

## 4. 成長スケジュール（K をどんどん増やす）

### 4.1 例: 4段階成長
- Stage0: K=32（安定性確認）
- Stage1: K=64
- Stage2: K=128
- Stage3: K=256

### 4.2 追加のタイミング（目安）
次が揃ったら追加:
- 学習損失が滑らか（振動が小さい）
- 勾配ノルムが急上昇していない
- \(\beta\) 分布が1〜2個に極端集中し続けない

### 4.3 追加のやり方
- 系統1/2/6（安定系）を増やしやすい
- 系統5（SoftSort/Sinkhorn等）は少数から（重い・不安定になりやすい）

---

## 5. 剪定（後から軽くする：LTH的 “当たり抽出”）

### 5.1 演算子単位の剪定指標
- \(|\beta_k|\) の小さいものを削除
- もしくは平均寄与 \(\mathbb{E}[|\beta_k\,\mathrm{Norm}(f_k(h))|]\) を測って小さいものを削除

### 5.2 手順
1) 学習完了モデルで寄与を計測
2) 下位 \(q\%\) を削除（例: 50%）
3) 残りで微調整（数エポック）

---

## 6. 評価設計（研究として成立させる）

### 6.1 比較ベースライン（最低限）
- 同程度の計算量・パラメータの通常ネット（MLP/Conv/Transformer等）
- 演算子数が小さい版（K=32固定）
- 成長なし（K固定） vs 成長あり（K倍々）

### 6.2 アブレーション（“多様性が効いた”を示す）
- 系統ごとに削る:  
  - 周波数系なし  
  - 多項式/ゲートなし  
  - 拡散系なし  
  - Soft手続き系なし  
  - soft-poolなし  
- 連続パラメータの分布を狭める vs 広げる（多様性の寄与確認）
- Norm の有無（壊れやすさを定量化）

### 6.3 指標
- 精度/損失の最終値
- 収束速度（同じ計算 budget でどこまで行くか）
- 安定性（seed間分散、学習曲線の振動、勾配ノルム）
- 剪定後の性能維持率（当たり演算子が抽出できているか）

---

## 7. 失敗パターンと対策

1) 1つの演算子が支配（\(\beta\) が単一に集中）
- 対策: Norm 強化、\(\beta\) への正則化、温度/スケールの再設計、\(\gamma\) を下げる

2) 学習が不安定（損失が爆発）
- 対策: \(\gamma\) を下げる、学習率を下げる、演算子追加の頻度を落とす、系統5を減らす

3) 演算子を増やしても性能が伸びない
- 対策: 「多様性」が同質になっている可能性  
  → 周波数スケールや温度の分布を広げる、ランダムプログラム合成を導入、共有計算で増やせる軸を増やす

---

## 8. 次に書くと良い“実装仕様書”の項目（必要なら）

- タスク別（画像/時系列/表）に、ミニライブラリK=32の具体演算子セット
- 演算子生成の乱数分布（対数一様など）の標準化
- 共有計算グラフの詳細（どこを共有して安く増やすか）
- 推奨ハイパラ（\(\gamma\), \(\lambda_1\), dropout率 p, 成長タイミング）

