# ROS/ALTH 検討メモ 01: 全体像（仮説・アーキテクチャ）

作成日: 2025-12-29  
対象: 「重みの宝くじ仮説」を **“演算子（写像）・手続き” の宝くじ**に拡張し、**微分可能**かつ **離散的な選択（top-k ルーティング等）を使わない**新アーキテクチャ案を具体化する。

---

## 0. 目的

- 深層学習の「ランダム初期化＋学習」に対し、  
  **重みの多様性**だけでなく **演算子（写像）の多様性**を大きく増やすことで、
  - 学習を **より効率的**（早く・安定に・少ない調整で）にする
  - かつ **表現能力**（線形を超えた変換の“当たり”）を初期状態から多く含む
  ことを狙う。

---

## 1. 中心仮説（ALＴＨ: Algorithm Lottery Ticket Hypothesis）

> 十分大きい「微分可能な演算子（写像）バンク」を持つスーパー・ネットには、  
> 学習前からタスクに有効な小さな “当たり構成”（当たり写像の合成）が含まれており、  
> 学習は主に **それらの連続的な合成係数**を通じて有効写像を強めていく。

ポイント:
- **“どれを使うか”を離散的に選ばない**（不安定化しやすいので避ける）
- 代わりに、**全部を並列に計算して連続係数で重ね合わせる**
- 結果として、係数が小さくなる演算子は **“勝手に使われなくなる”**（後から剪定も可）

---

## 2. アーキテクチャ（提案）: Operator Basis Layer（選択なし・全混合）

### 2.1 記号（変数定義）
- バッチサイズ: \(B\)
- 特徴次元: \(d\)
- 層入力（中間表現）: \(h \in \mathbb{R}^{B\times d}\)
- 演算子数: \(K\)
- 演算子（微分可能写像）:  
  \[
  f_k(\cdot;\omega_k): \mathbb{R}^{B\times d}\to\mathbb{R}^{B\times d}\quad (k=1..K)
  \]
  - \(\omega_k\): 演算子内部の乱数・定数（**固定**を基本、必要なら一部学習可）
- 正規化: \(\mathrm{Norm}(\cdot)\)（LayerNorm / RMSNorm 等。基本 **必須**）
- 合成係数: \(\beta_k\)（学習パラメータ）
- 残差スケール（任意）: \(\gamma\)（学習/固定どちらでも可）

### 2.2 層更新式（残差付き）
\[
h' \,=\, h\; +\; \gamma\sum_{k=1}^{K} \beta_k\, \mathrm{Norm}\big(f_k(h;\omega_k)\big)
\]

解釈:
- 各 \(f_k\) は “候補写像”
- \(\beta_k\) がその寄与度（連続的・微分可能）
- **離散ルーティングはしない**（top-k / switch 等なし）

### 2.3 なぜ Norm が必須か
演算子を増やすと、
- 出力分散が大きい演算子が支配する
- 勾配が壊れる（爆発/消失）
- 学習が不安定

この事故を避けるため、原則として
- **各演算子出力を混合前に Norm する**
- 追加演算子は \(\beta_k\approx 0\) で **弱く足す**

---

## 3. 「どんどん増やす」を成立させる運用（成長スケジュール）

### 3.1 演算子の追加ルール
- 新規演算子 \(f_{new}\) を追加するとき:
  - 係数 \(\beta_{new}\) を **0 または微小値**で初期化
  - 既存モデル挙動を壊さず “必要なら立ち上がる” ようにする

### 3.2 演算子数の成長（例）
- Stage 0: \(K=32\)（安定化のため小さく開始）
- Stage 1: \(K=64\)
- Stage 2: \(K=128\)
- Stage 3: \(K=256\)

増やす条件（目安）:
- 学習曲線が安定（損失が滑らか）
- 勾配ノルムが過剰に増えていない
- 係数分布 \(\{\beta_k\}\) が一部に極端集中しない

---

## 4. 計算量を抑える（選択に戻らない前提）

**原則**: 離散選択に戻らずに計算量を落とす。

1) 共有計算の徹底  
- 例: 同じ線形変換 \(hW\) を複数の非線形（sin/cos/多項式/ゲート）で使い回す  
- 例: 同じ畳み込み出力を複数の温度 \(\tau\) の soft-pool で再利用

2) Operator Dropout（学習時の確率的間引き）  
- 各 step で一部の演算子を確率 \(p\) で無効化（期待値的には全混合に近い）
- これは **ルーティング学習**ではなく、計算節約＋正則化

---

## 5. 研究としての主張（最小セット）

- **主張A**: 演算子の “事前分布（operator prior）” の設計が学習効率を決める  
- **主張B**: 微分可能な “手続きの連続緩和” を演算子として注入すると、表現が豊かになる  
- **主張C**: 離散選択なしでも、係数学習により有効演算子が自然に浮上し、後から剪定できる

---

## 6. 次ファイル案内

- `ROS-ALTH_02_operator_library.md`  
  演算子ライブラリを「微分可能」という縛りのまま大量に増やすための具体仕様（系統・生成ルール・乱数分布・共有計算設計）。

- `ROS-ALTH_03_training_evaluation.md`  
  学習手順（正規化、初期化、正則化、成長スケジュール、剪定）と、検証プロトコル（アブレーション、指標、失敗パターンと対策）。
